{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Welcome to PDF2JSON converter\n",
        "\n",
        "This notebook provides all the neccessary information and code to understand this part."
      ],
      "metadata": {
        "id": "pbO7Oz8UMpBZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zx8J55enMkxe"
      },
      "outputs": [],
      "source": [
        "!pip install colab-xterm\n",
        "%load_ext colabxterm #using external terminal to use ollama within notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process\n",
        "\n",
        "User is required to use the following steps to use large Langauge Model (Mistral)\n",
        "\n",
        "- Run the following command to install ollama\n",
        "\n",
        "```curl -fsSL https://ollama.com/install.sh | sh```\n",
        "\n",
        "- Serve and pull the LLM\n",
        "\n",
        "``` ollama serve & ollama pull mistral```\n",
        "\n",
        "- Run the LLM using this command\n",
        "\n",
        "```ollama run mistral```"
      ],
      "metadata": {
        "id": "qokuiPX_NHgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%xterm #running external terminal"
      ],
      "metadata": {
        "id": "v7ZivfMbNDFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# installing tesseract-ocr in runtime and other libraries for the use\n",
        "\n",
        "!apt-get install poppler-utils tesseract-ocr\n",
        "!pip install pytesseract pdf2image PyPDF2 transformers langchain\n",
        "!pip install langchain-community"
      ],
      "metadata": {
        "id": "JrKeciTsNtMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import pytesseract\n",
        "from pdf2image import convert_from_path\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.llms import Ollama"
      ],
      "metadata": {
        "id": "OeKEoqMpO88k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extraction using PyPDF2 (even pdfplumber could be used for the purpose)\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    reader = PdfReader(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        page_text = page.extract_text()\n",
        "        if page_text:\n",
        "            text += page_text + \"\\n\"\n",
        "    return text"
      ],
      "metadata": {
        "id": "7x_7nZ1nPCy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use of tesseract, if the pdf contains scanned images\n",
        "\n",
        "# note that, here it works without this, because the data contains no images\n",
        "def ocr_from_images(pdf_path):\n",
        "    images = convert_from_path(pdf_path)\n",
        "    extracted_text = \"\"\n",
        "    for img in images:\n",
        "        text = pytesseract.image_to_string(img)\n",
        "        extracted_text += text + \"\\n\"\n",
        "    return extracted_text.strip()"
      ],
      "metadata": {
        "id": "q87dPSy-PJxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating chunks\n",
        "def split_text_into_chunks(text, chunk_size=1000):\n",
        "    \"\"\"Split text into smaller chunks if it's too long for the model to handle.\"\"\"\n",
        "    lines = text.split('\\n')\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "    for line in lines:\n",
        "        if len(current_chunk) + len(line) < chunk_size:\n",
        "            current_chunk += line + \"\\n\"\n",
        "        else:\n",
        "            chunks.append(current_chunk)\n",
        "            current_chunk = line + \"\\n\"\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk)\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "VFfRy0JSPZ-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load mistral model\n",
        "def load_llama_model():\n",
        "    llm = Ollama(model=\"mistral\")\n",
        "    return llm"
      ],
      "metadata": {
        "id": "Hw6nGDd3PeSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompting llm to generate structured json\n",
        "def intelligent_structure_with_llm(text, llm):\n",
        "    chunks = split_text_into_chunks(text)\n",
        "    structured_data = {}\n",
        "\n",
        "    for chunk in chunks:\n",
        "        prompt = f\"\"\"\n",
        "        Here is some text extracted from a PDF. Analyze it and generate structured JSON where main sections are parent keys and subsections are children.\n",
        "\n",
        "        Text:\n",
        "        {chunk}\n",
        "\n",
        "        Now provide the structured JSON:\n",
        "        \"\"\"\n",
        "        response = llm(prompt)\n",
        "        try:\n",
        "            json_start = response.find(\"{\")\n",
        "            json_end = response.rfind(\"}\") + 1\n",
        "            structured_json = response[json_start:json_end]\n",
        "            chunk_data = json.loads(structured_json)\n",
        "            structured_data.update(chunk_data)  # merge the structured data\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"Error decoding JSON from chunk:\\n{response}\")\n",
        "\n",
        "    return structured_data"
      ],
      "metadata": {
        "id": "3cXv9zekPl3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_pdf_to_json_with_llama(pdf_path):\n",
        "    extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    # OCR working condition\n",
        "    if not extracted_text.strip():\n",
        "        print(\"No text found using PyPDF2. Falling back to Tesseract OCR.\")\n",
        "        extracted_text = ocr_from_images(pdf_path)\n",
        "\n",
        "    if not extracted_text:\n",
        "        print(\"No text could be recognized.\")\n",
        "        return\n",
        "\n",
        "\n",
        "    llm = load_llama_model()\n",
        "\n",
        "    structured_json = intelligent_structure_with_llm(extracted_text, llm)\n",
        "\n",
        "    if not structured_json:\n",
        "        print(\"No valid JSON could be generated.\")\n",
        "        return\n",
        "\n",
        "    output_json_path = pdf_path.replace('.pdf', '.json')\n",
        "    with open(output_json_path, 'w') as json_file:\n",
        "        json.dump(structured_json, json_file, indent=4)\n",
        "\n",
        "    print(f\"Structured JSON saved to: {output_json_path}\")"
      ],
      "metadata": {
        "id": "FTGzXZiaPzzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    pdf_file_path = '/content/ammonium-hydroxide-acs-lb.pdf'  # Replace with your PDF file path\n",
        "    process_pdf_to_json_with_llama(pdf_file_path)"
      ],
      "metadata": {
        "id": "ezRiEjbOQKIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If bulk folders are needed to be processed, do the following\n",
        "\n",
        "- uncomment this code\n",
        "- replace the following in the main execution part from ```pdf_file_path``` with ```folder_path```"
      ],
      "metadata": {
        "id": "HwbuPFVCRPmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def process_folder_of_pdfs(folder_path):\n",
        "#     for f in os.listdir(folder_path):\n",
        "#         if f.endswith('.pdf'):\n",
        "#             pdf_files.append(f)\n",
        "#     if not pdf_files:\n",
        "#         print(\"No PDF files found in the folder.\")\n",
        "#         return\n",
        "\n",
        "#     for pdf_file in pdf_files:\n",
        "#         pdf_path = os.path.join(folder_path, pdf_file)\n",
        "#         print(f\"Processing {pdf_path}...\")\n",
        "#         process_pdf_to_json_with_llama(pdf_path)\n",
        "\n",
        "#     print(\"All PDFs have been processed.\")"
      ],
      "metadata": {
        "id": "9-Xjpzd_Rj1j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}